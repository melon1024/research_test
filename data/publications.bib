
@article{park_deep_2023,
	title = {Deep {Neural} {Networks}-based {Classification} {Methodologies} of {Speech}, {Audio} and {Music}, and its {Integration} for {Audio} {Metadata} {Tagging}},
	copyright = {Copyright (c) 2023 Journal of Web Engineering},
	issn = {1544-5976},
	url = {https://journals.riverpublishers.com/index.php/JWE/},
	doi = {10.13052/jwe1540-9589.2211},
	abstract = {Videos contain visual and auditory information. Visual information in a video can include images of people, objects, and the landscape, whereas auditory information includes voices, sound effects, background music, and the soundscape. The audio content can provide detailed information on the story by conducting a voice and atmosphere analysis of the sound effects and soundscape. Metadata tags represent the results of a media analysis as text. The tags can classify video content on social networking services, like YouTube. This paper presents the methodologies of speech, audio, and music processing. Also, we propose integrating these audio tagging methods and applying them in an audio metadata generation system for video storytelling. The proposed system automatically creates metadata tags based on speech, sound effects, and background music information from the audio input. The proposed system comprises five subsystems: (1) automatic speech recognition, which generates text from the linguistic sounds in the audio, (2) audio event classification for the type of sound effect, (3) audio scene classification for the type of place from the soundscape, (4) music detection for the background music, and (5) keyword extraction from the automatic speech recognition results. First, the audio signal is converted into a suitable form, which is subsequently combined from each subsystem to create metadata for the audio content. We evaluated the proposed system using video logs (vlogs) on YouTube. The proposed system exhibits a similar accuracy to handcrafted metadata for the audio content, and for a total of 104 YouTube vlogs, achieves an accuracy of 65.83\%.},
	language = {en},
	urldate = {2024-04-11},
	journal = {Journal of Web Engineering},
	author = {Park, Hosung and Chung, Yoonseo and Kim, Ji-Hwan},
	month = apr,
	year = {2023},
	keywords = {audio scene classification},
	pages = {1--26},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\B8N9F4E7\\Park 등 - 2023 - Deep Neural Networks-based Classification Methodol.pdf:application/pdf},
}

@article{seo_tp-mobnet_2022,
	title = {{TP}-{MobNet}: {A} {Two}-pass {Mobile} {Network} for {Low}-complexity {Classification} of {Acoustic} {Scene}},
	volume = {73},
	issn = {1546-2218, 1546-2226},
	shorttitle = {{TP}-{MobNet}},
	url = {https://www.techscience.com/cmc/v73n2/48321},
	doi = {10.32604/cmc.2022.026259},
	abstract = {Acoustic scene classification (ASC) is a method of recognizing and classifying environments that employ acoustic signals. Various ASC approaches based on deep learning have been developed, with convolutional neural networks (CNNs) proving to be the most reliable and commonly utilized in ASC systems due to their suitability for constructing lightweight models. When using ASC systems in the real world, model complexity and device robustness are essential considerations. In this paper, we propose a two-pass mobile network for low-complexity classification of the acoustic scene, named TP-MobNet. With inverse residuals and linear bottlenecks, TP-MobNet is based on MobileNetV2, and following mobile blocks, coordinate attention and two-pass fusion approaches are utilized. The log-range dependencies and precise position information in feature maps can be trained via coordinate attention. By capturing more diverse feature resolutions at the network's end sides, two-pass fusions can also train generalization. Also, the model size is reduced by applying weight quantization to the trained model. By adding weight quantization to the trained model, the model size is also lowered. The TAU Urban Acoustic Scenes 2020 Mobile development set was used for all of the experiments. It has been confirmed that the proposed model, with a model size of 219.6 kB, achieves an accuracy of 73.94\%.},
	language = {en},
	number = {2},
	urldate = {2024-04-11},
	journal = {Computers, Materials \& Continua},
	author = {Seo, Soonshin and Oh, Junseok and Cho, Eunsoo and Park, Hosung and Kim, Gyujin and Kim, Ji-Hwan},
	year = {2022},
	note = {Publisher: Tech Science Press},
	pages = {3291--3303},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\Y8CEZC8C\\Seo 등 - 2022 - TP-MobNet A Two-pass Mobile Network for Low-compl.pdf:application/pdf},
}

@article{park_hybrid_2022,
	title = {Hybrid {CTC}-{Attention} {Network}-{Based} {End}-to-{End} {Speech} {Recognition} {System} for {Korean} {Language}},
	copyright = {Copyright (c) 2022 Journal of Web Engineering},
	issn = {1544-5976},
	url = {https://journals.riverpublishers.com/index.php/JWE/},
	doi = {10.13052/jwe1540-9589.2126},
	abstract = {In this study, an automatic end-to-end speech recognition system based on hybrid CTC-attention network for Korean language is proposed. Deep neural network/hidden Markov model (DNN/HMM)-based speech recognition system has driven dramatic improvement in this area. However, it is difficult for non-experts to develop speech recognition for new applications. End-to-end approaches have simplified speech recognition system into a single-network architecture. These approaches can develop speech recognition system that does not require expert knowledge. In this paper, we propose hybrid CTC-attention network as end-to-end speech recognition model for Korean language. This model effectively utilizes a CTC objective function during attention model training. This approach improves the performance in terms of speech recognition accuracy as well as training speed. In most languages, end-to-end speech recognition uses characters as output labels. However, for Korean, character-based end-to-end speech recognition is not an efficient approach because Korean language has 11,172 possible numbers of characters. The number is relatively large compared to other languages. For example, English has 26 characters, and Japanese has 50 characters. To address this problem, we utilize Korean 49 graphemes as output labels. Experimental result shows 10.02\% character error rate (CER) when 740 hours of Korean training data are used.},
	language = {en},
	urldate = {2024-04-11},
	journal = {Journal of Web Engineering},
	author = {Park, Hosung and Kim, Changmin and Son, Hyunsoo and Seo, Soonshin and Kim, Ji-Hwan},
	month = jan,
	year = {2022},
	keywords = {Korean speech recognition},
	pages = {265--284},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\MZ5JEFFX\\Park 등 - 2022 - Hybrid CTC-Attention Network-Based End-to-End Spee.pdf:application/pdf},
}

@article{son_conformer_2021,
	title = {Conformer with lexicon transducer for {Korean} end-to-end speech recognition},
	volume = {40},
	issn = {1225-4428, 2287-3775},
	url = {https://www.jask.or.kr/articles/xml/v4XO/www.jask.or.kr/articles/article/v4XO/},
	doi = {10.7776/ASK.2021.40.5.530},
	language = {ko-KR},
	number = {5},
	urldate = {2024-04-11},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Son, Hyunsoo and Park, Hosung and Kim, Gyujin and Cho, Eunsoo and Kim, Ji-Hwan},
	month = sep,
	year = {2021},
	note = {Publisher: The Acoustical Society Of Korea},
	pages = {530--536},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\SWS3J39V\\Son 등 - 2021 - Conformer with lexicon transducer for Korean end-t.pdf:application/pdf},
}

@article{lee_language_2021,
	title = {Language {Model} {Using} {Differentiable} {Neural} {Computer} {Based} on {Forget} {Gate}-{Based} {Memory} {Deallocation}},
	volume = {68},
	issn = {1546-2218, 1546-2226},
	url = {https://www.techscience.com/cmc/v68n1/41815},
	doi = {10.32604/cmc.2021.015430},
	abstract = {A differentiable neural computer (DNC) is analogous to the Von Neumann machine with a neural network controller that interacts with an external memory through an attention mechanism. Such DNC’s offer a generalized method for task-specific deep learning models and have demonstrated reliability with reasoning problems. In this study, we apply a DNC to a language model (LM) task. The LM task is one of the reasoning problems, because it can predict the next word using the previous word sequence. However, memory deallocation is a problem in DNCs as some information unrelated to the input sequence is not allocated and remains in the external memory, which degrades performance. Therefore, we propose a forget gate-based memory deallocation (FMD) method, which searches for the minimum value of elements in a forget gate-based retention vector. The forget gate-based retention vector indicates the retention degree of information stored in each external memory address. In experiments, we applied our proposed NTM architecture to LM tasks as a task-specific example and to rescoring for speech recognition as a general-purpose example. For LM tasks, we evaluated DNC using the Penn Treebank and enwik8 LM tasks. Although it does not yield SOTA results in LM tasks, the FMD method exhibits relatively improved performance compared with DNC in terms of bits-per-character. For the speech recognition rescoring tasks, FMD again showed a relative improvement using the LibriSpeech data in terms of word error rate.},
	language = {en},
	number = {1},
	urldate = {2024-04-11},
	journal = {Computers, Materials \& Continua},
	author = {Lee, Donghyun and Park, Hosung and Seo, Soonshin and Kim, Changmin and Son, Hyunsoo and Kim, Gyujin and Kim, Ji-Hwan},
	year = {2021},
	note = {Publisher: Tech Science Press},
	pages = {537--551},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\A25MIFIH\\Lee 등 - 2021 - Language Model Using Differentiable Neural Compute.pdf:application/pdf},
}

@article{lee_robustness_2021,
	title = {Robustness of {Differentiable} {Neural} {Computer} {Using} {Limited} {Retention} {Vector}-based {Memory} {Deallocation} in {Language} {Model}},
	volume = {15},
	issn = {1976-7277},
	url = {https://koreascience.kr/article/JAKO202116739524293.page},
	doi = {10.3837/tiis.2021.03.002},
	abstract = {Recurrent neural network (RNN) architectures have been used for language modeling (LM) tasks that require learning long-range word or character sequences. However, the RNN architecture is still suffered from unstable gradients on long-range sequences. To address the issue of long-range sequences, an attention mechanism has been used, showing state-of-the-art (SOTA) performance in all LM tasks. A differentiable neural computer (DNC) is a deep learning architecture using an attention mechanism. The DNC architecture is a neural network augmented with a content-addressable external memory. However, in the write operation, some information unrelated to the input word remains in memory. Moreover, DNCs have been found to perform poorly with low numbers of weight parameters. Therefore, we propose a robust memory deallocation method using a limited retention vector. The limited retention vector determines whether the network increases or decreases its usage of information in external memory according to a threshold. We experimentally evaluate the robustness of a DNC implementing the proposed approach according to the size of the controller and external memory on the enwik8 LM task. When we decreased the number of weight parameters by 32.47\%, the proposed DNC showed a low bits-per-character (BPC) degradation of 4.30\%, demonstrating the effectiveness of our approach in language modeling tasks.},
	language = {eng},
	number = {3},
	urldate = {2024-04-11},
	journal = {KSII Transactions on Internet and Information Systems (TIIS)},
	author = {Lee, Donghyun and Park, Hosung and Seo, Soonshin and Son, Hyunsoo and Kim, Gyujin and Kim, Ji-Hwan},
	year = {2021},
	note = {Publisher: Korean Society for Internet Information},
	pages = {837--852},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\92CHECR2\\Lee 등 - 2021 - Robustness of Differentiable Neural Computer Using.pdf:application/pdf},
}

@article{seo_self-attentive_2020,
	title = {Self-{Attentive} {Multi}-{Layer} {Aggregation} with {Feature} {Recalibration} and {Deep} {Length} {Normalization} for {Text}-{Independent} {Speaker} {Verification} {System}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/9/10/1706},
	doi = {10.3390/electronics9101706},
	abstract = {One of the most important parts of a text-independent speaker verification system is speaker embedding generation. Previous studies demonstrated that shortcut connections-based multi-layer aggregation improves the representational power of a speaker embedding system. However, model parameters are relatively large in number, and unspecified variations increase in the multi-layer aggregation. Therefore, in this study, we propose a self-attentive multi-layer aggregation with feature recalibration and deep length normalization for a text-independent speaker verification system. To reduce the number of model parameters, we set the ResNet with the scaled channel width and layer depth as a baseline. To control the variability in the training, we apply a self-attention mechanism to perform multi-layer aggregation with dropout regularizations and batch normalizations. Subsequently, we apply a feature recalibration layer to the aggregated feature using fully-connected layers and nonlinear activation functions. Further, deep length normalization is used on a recalibrated feature in the training process. Experimental results using the VoxCeleb1 evaluation dataset showed that the performance of the proposed methods was comparable to that of state-of-the-art models (equal error rate of 4.95\% and 2.86\%, using the VoxCeleb1 and VoxCeleb2 training datasets, respectively).},
	language = {en},
	number = {10},
	urldate = {2024-04-11},
	journal = {Electronics},
	author = {Seo, Soonshin and Kim, Ji-Hwan},
	month = oct,
	year = {2020},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ResNet, convolutional neural networks, deep length normalization, feature recalibration, multi-layer aggregation, self-attentive pooling, shortcut connections, speaker embedding, text-independent speaker verification system},
	pages = {1706},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\ZN5ZUN73\\Seo 그리고 Kim - 2020 - Self-Attentive Multi-Layer Aggregation with Featur.pdf:application/pdf},
}

@article{seo_masked_2020,
	title = {Masked cross self-attentive encoding based speaker embedding for speaker verification},
	volume = {39},
	issn = {1225-4428, 2287-3775},
	url = {https://www.jask.or.kr/articles/xml/wO4y/www.jask.or.kr/articles/article/wO4y/},
	doi = {10.7776/ASK.2020.39.5.497},
	language = {ko-KR},
	number = {5},
	urldate = {2024-04-11},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Seo, Soonshin and Kim, Ji-Hwan},
	month = sep,
	year = {2020},
	note = {Publisher: The Acoustical Society Of Korea},
	pages = {497--504},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\FSNDBQSI\\Seo 그리고 Kim - 2020 - Masked cross self-attentive encoding based speaker.pdf:application/pdf},
}

@article{park_acoustic_2020,
	title = {Acoustic model training using self-attention for low-resource speech recognition},
	volume = {39},
	issn = {1225-4428, 2287-3775},
	url = {https://www.jask.or.kr/articles/xml/Ljzg/www.jask.or.kr/articles/article/Ljzg/},
	doi = {10.7776/ASK.2020.39.5.483},
	language = {ko-KR},
	number = {5},
	urldate = {2024-04-11},
	journal = {The Journal of the Acoustical Society of Korea},
	author = {Park, Hosung and Kim, Ji-Hwan},
	month = sep,
	year = {2020},
	note = {Publisher: The Acoustical Society Of Korea},
	pages = {483--489},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\HB7SJD8F\\Park 그리고 Kim - 2020 - Acoustic model training using self-attention for l.pdf:application/pdf},
}

@article{seo_convolutional_2022,
	title = {Convolutional {Neural} {Networks} {Using} {Log} {Mel}-{Spectrogram} {Separation} for {Audio} {Event} {Classification} with {Unknown} {Devices}},
	copyright = {Copyright (c) 2022 Journal of Web Engineering},
	issn = {1544-5976},
	url = {https://journals.riverpublishers.com/index.php/JWE/},
	doi = {10.13052/jwe1540-9589.21216},
	abstract = {Audio event classification refers to the detection and classification of non-verbal signals, such as dog and horn sounds included in audio data, by a computer. Recently, deep neural network technology has been applied to audio event classification, exhibiting higher performance when compared to existing models. Among them, a convolutional neural network (CNN)-based training method that receives audio in the form of a spectrogram, which is a two-dimensional image, has been widely used. However, audio event classification has poor performance on test data when it is recorded by a device (unknown device) different from that used to record training data (known device). This is because the frequency range emphasized is different for each device used during recording, and the shapes of the resulting spectrograms generated by known devices and those generated by unknown devices differ. In this study, to improve the performance of the event classification system, a CNN based on the log mel-spectrogram separation technique was applied to the event classification system, and the performance of unknown devices was evaluated. The system can classify 16 types of audio signals. It receives audio data at 0.4-s length, and measures the accuracy of test data generated from unknown devices with a model trained via training data generated from known devices. The experiment showed that the performance compared to the baseline exhibited a relative improvement of up to 37.33\%, from 63.63\% to 73.33\% based on Google Pixel, and from 47.42\% to 65.12\% based on the LG V50.},
	language = {en},
	urldate = {2024-04-16},
	journal = {Journal of Web Engineering},
	author = {Seo, Soonshin and Kim, Changmin and Kim, Ji-Hwan},
	month = jan,
	year = {2022},
	keywords = {convolutional neural networks},
	pages = {497--522},
	file = {Full Text PDF:C\:\\Users\\user\\Zotero\\storage\\2A7KSP9K\\Seo 등 - 2022 - Convolutional Neural Networks Using Log Mel-Spectr.pdf:application/pdf},
}
